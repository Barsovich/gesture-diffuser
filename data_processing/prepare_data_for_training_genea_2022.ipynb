{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/barissen/projects/gesture-diffuser/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import argparse\n",
    "from os import listdir, mkdir\n",
    "from os.path import isfile, join, exists\n",
    "import math\n",
    "\n",
    "from pymo.preprocessing import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pymo.parsers import BVHParser\n",
    "from pymo.viz_tools import *\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_dir = \"../data/GENEA_2022/val/bvh/\"\n",
    "transcript_dir = \"../data/GENEA_2022/val/tsv/\"\n",
    "transcript_out_dir = \"../data/GENEA_2022/val/json/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_files = [f for f in listdir(\n",
    "    motion_dir) if isfile(join(motion_dir, f)) and f[-3:] == \"bvh\"]\n",
    "recording_files.sort()\n",
    "motion_files = [join(motion_dir, recording_files[i]) for i in range(len(recording_files))]\n",
    "transcript_files = [join(transcript_dir, recording_files[i].replace(\n",
    "    'bvh', 'tsv')) for i in range(len(recording_files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_size = 5 # s\n",
    "words_per_half_second = 15\n",
    "num_offsets = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sectionize(transcript_files, clip_size, start_offset=0):\n",
    "  all_transcripts = []\n",
    "  for file_id in range(len(transcript_files)):\n",
    "    try:\n",
    "      transcript = pd.read_csv(transcript_files[file_id], sep='\\t', header=None)\n",
    "    except pd.errors.EmptyDataError:\n",
    "      print(f\"File {transcript_files[file_id]} was empty.\")\n",
    "      all_transcripts.append([])\n",
    "      continue\n",
    "    \n",
    "    # Sectionize the words\n",
    "    if not exists(transcript_out_dir):\n",
    "      mkdir(transcript_out_dir)\n",
    "    \n",
    "    words_sectioned = []\n",
    "    current_clip_start = start_offset\n",
    "    words_counter = 0\n",
    "    max_tokens_per_section = 0\n",
    "\n",
    "    # Set the sections empty until there the first word occurance\n",
    "    first_detected_word_start = transcript.iloc[words_counter, 0]\n",
    "    while first_detected_word_start > current_clip_start + clip_size:\n",
    "      words_sectioned += [{\"start\": current_clip_start,\n",
    "                          \"end\": current_clip_start + clip_size, \n",
    "                          \"indices\": [],\n",
    "                          \"section_text\": \"\"}]\n",
    "      current_clip_start += clip_size\n",
    "\n",
    "    while words_counter < transcript.shape[0]:\n",
    "      # Add all words in the section to a list\n",
    "      words_in_section = []\n",
    "      word_indices_in_section = []\n",
    "      current_index = 0\n",
    "      current_section = 0\n",
    "\n",
    "      while words_counter < transcript.shape[0] and transcript.iloc[words_counter, 0] < current_clip_start + clip_size:\n",
    "        word_with_possible_punctuation = transcript.iloc[words_counter, 2]\n",
    "        if not isinstance(word_with_possible_punctuation, str):\n",
    "          word_with_possible_punctuation = 'uh'\n",
    "        words_in_section.append(word_with_possible_punctuation)\n",
    "        words_tokenized = tokenizer.tokenize(word_with_possible_punctuation)\n",
    "        num_tokens = len(words_tokenized)\n",
    "        assert num_tokens == len(tokenizer(word_with_possible_punctuation)['input_ids']) - 2, \\\n",
    "          f\"{word_with_possible_punctuation}, {num_tokens}, {words_tokenized}, {tokenizer(word_with_possible_punctuation)}\"\n",
    "        section_within_section = np.floor((transcript.iloc[words_counter, 0] - current_clip_start) * 2).astype('int32')\n",
    "        if section_within_section > current_section:\n",
    "          current_section = int(section_within_section)\n",
    "          current_index = current_section * words_per_half_second\n",
    "        \n",
    "        for token in words_tokenized:\n",
    "          word_indices_in_section.append(current_index)\n",
    "          current_index += 1\n",
    "          words_counter += 1\n",
    "      \n",
    "      section_text = \" \".join(words_in_section)\n",
    "      assert len(tokenizer.tokenize(section_text)) == len(word_indices_in_section), \\\n",
    "        f\"{word_indices_in_section}, {tokenizer.tokenize(section_text)}, {section_text}\"\n",
    "      max_tokens_per_section = max(\n",
    "          max_tokens_per_section, len(word_indices_in_section))\n",
    "      words_sectioned += [{\"start\": current_clip_start,\n",
    "                          \"end\": current_clip_start + clip_size, \n",
    "                          \"indices\": word_indices_in_section, \n",
    "                          \"section_text\": section_text}]\n",
    "      current_clip_start += clip_size\n",
    "    all_transcripts.append(words_sectioned)\n",
    "  return all_transcripts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_transcripts = [sectionize(transcript_files, clip_size, start_offset=i / 2) for i in range(num_offsets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max words in a section: 45\n"
     ]
    }
   ],
   "source": [
    "word_counts = []\n",
    "for i in range(len(all_transcripts)):\n",
    "  offset = all_transcripts[i]\n",
    "  for j in range(len(offset)):\n",
    "    transcript = offset[j]\n",
    "    for k in range(len(transcript)):\n",
    "      section = transcript[k]\n",
    "      word_counts.append(len(section[\"indices\"]))\n",
    "print(f\"Max words in a section: {max(word_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_offsets):\n",
    "  with open(join(transcript_out_dir, f\"text_{clip_size}s_offset_{i}_half_s.json\"), \"w\") as f:\n",
    "    json.dump(all_transcripts[i], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 277\n",
      "[3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 10, 14]\n"
     ]
    }
   ],
   "source": [
    "def check(transcript_files, clip_size, start_offset=0):\n",
    "  all_transcripts = []\n",
    "  max_counts = []\n",
    "  for file_id in range(len(transcript_files)):\n",
    "    try:\n",
    "      transcript = pd.read_csv(transcript_files[file_id], sep='\\t', header=None)\n",
    "    except pd.errors.EmptyDataError:\n",
    "      print(f\"File {transcript_files[file_id]} was empty.\")\n",
    "      all_transcripts.append([])\n",
    "      continue\n",
    "    \n",
    "    words_sectioned = []\n",
    "    current_clip_start = start_offset\n",
    "    words_counter = 0\n",
    "\n",
    "    word_starts = np.floor(np.array(transcript.iloc[:, 0]) * 2).astype('int32')\n",
    "    word_starts_tokenized = []\n",
    "    for i in range(len(word_starts)):\n",
    "      text = transcript.iloc[i, 2]\n",
    "      if not isinstance(text, str):\n",
    "        text = \"uh\"\n",
    "      word_starts_tokenized += [word_starts[i]] * len(tokenizer.tokenize(text))\n",
    "    \n",
    "    word_starts_tokenized_np = np.array(word_starts_tokenized)\n",
    "    unique, counts = np.unique(word_starts_tokenized_np, return_counts=True)\n",
    "    max_counts.append(np.max(counts))\n",
    "  print(max(max_counts), np.argmax(max_counts))\n",
    "  print(sorted(max_counts))\n",
    "   \n",
    "check(transcript_files, clip_size, start_offset=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(\"a\")['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': [101, 4148, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]},\n",
       " {'input_ids': [101, 2711, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]},\n",
       " {'input_ids': [101, 2043, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]},\n",
       " {'input_ids': [101, 2027, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]},\n",
       " {'input_ids': [101, 2031, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]},\n",
       " {'input_ids': [101, 2066, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]},\n",
       " {'input_ids': [101, 2070, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]},\n",
       " {'input_ids': [101, 2111, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]},\n",
       " {'input_ids': [101, 2031, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]},\n",
       " {'input_ids': [101, 1037, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]},\n",
       " {'input_ids': [101, 6669, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]},\n",
       " {'input_ids': [101, 2166, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]},\n",
       " {'input_ids': [101, 2007, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]},\n",
       " {'input_ids': [101, 2053, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]},\n",
       " {'input_ids': [101, 2498, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]},\n",
       " {'input_ids': [101, 3308, 1012, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer(i) for i in \"happen person when they have like some people have a perfectly life with no nothing wrong.\".split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 4148, 2711, 2043, 2027, 2031, 2066, 2070, 2111, 2031, 1037, 6669, 2166, 2007, 2053, 2498, 3308, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"happen person when they have like some people have a perfectly life with no nothing wrong.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3919, 2147], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"no nothing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happen'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([71, 1324, 268, 1048, 618, 484, 423, 588, 617, 661, 423, 257, 7138, 1204, 351, 645, 2147, 2642, 13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 687kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 6.38kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 126kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'have', 'a', 'new', 'gp', '##u', '!']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer.tokenize(\"I have a new GPU!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fb60b568b9eafea3dac73304166ce4cbcca6e97c109218915796e4b74930d0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
